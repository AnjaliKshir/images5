You multiply two 8x8 matrices using GPU parallel computing (CUDA).
First, you create matrices A and B on CPU (host).
Then, you move them to GPU (device).
You launch a CUDA kernel where each thread computes one element of the result matrix C.
You copy back C from GPU to CPU and display it.

# (Important) first go to runtime -> change runtime type -> Select T4 - GPU (Any Available GPU) -> Save
!nvcc --version
     
# Debug log: Check whether the Runtime is GPU based or not!
import torch
torch.cuda.is_available() # Should be True

cuda_code = r"""
#include 
#include 

// Use long long int for calculations to prevent overflow
__global__ void matmul(int* A, int* B, long long int* C, int N) {
    int Row = blockIdx.y * blockDim.y + threadIdx.y;
    int Col = blockIdx.x * blockDim.x + threadIdx.x;
    if (Row < N && Col < N) {
        long long int Pvalue = 0;
        for (int k = 0; k < N; k++) {
            Pvalue += (long long int)A[Row * N + k] * B[k * N + Col];
        }
        C[Row * N + Col] = Pvalue;
    }
}

int main() {
    // Use a smaller matrix size for easier verification
    int N = 8;
    int size = N * N * sizeof(int);
    size_t result_size = N * N * sizeof(long long int);

    // Allocate CPU memory
    int* A = (int*)malloc(size);
    int* B = (int*)malloc(size);
    long long int* C = (long long int*)malloc(result_size);

    // Initialize matrices A and B with simple values
    // Matrix A: simple incremental values
    int A_data[64] = {
        1, 2, 3, 4, 5, 6, 7, 8,
        9, 10, 11, 12, 13, 14, 15, 16,
        17, 18, 19, 20, 21, 22, 23, 24,
        25, 26, 27, 28, 29, 30, 31, 32,
        33, 34, 35, 36, 37, 38, 39, 40,
        41, 42, 43, 44, 45, 46, 47, 48,
        49, 50, 51, 52, 53, 54, 55, 56,
        57, 58, 59, 60, 61, 62, 63, 64
    };

    // Matrix B: identity matrix for simplicity
    int B_data[64] = {
        1, 0, 0, 0, 0, 0, 0, 0,
        0, 1, 0, 0, 0, 0, 0, 0,
        0, 0, 1, 0, 0, 0, 0, 0,
        0, 0, 0, 1, 0, 0, 0, 0,
        0, 0, 0, 1, 1, 0, 0, 0,
        0, 0, 0, 0, 0, 1, 0, 0,
        0, 0, 0, 0, 0, 0, 1, 0,
        0, 0, 0, 0, 0, 0, 0, 1
    };

    // Copy the data to A and B
    memcpy(A, A_data, size);
    memcpy(B, B_data, size);

    // Print matrix A
    std::cout << "Matrix A:" << std::endl;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            std::cout << A[i * N + j] << " ";
        }
        std::cout << std::endl;
    }

    // Print matrix B
    std::cout << "\nMatrix B:" << std::endl;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            std::cout << B[i * N + j] << " ";
        }
        std::cout << std::endl;
    }

    // Allocate GPU memory
    int* dev_A, * dev_B;
    long long int* dev_C;
    cudaMalloc(&dev_A, size);
    cudaMalloc(&dev_B, size);
    cudaMalloc(&dev_C, result_size);

    // Copy A and B to device memory
    cudaMemcpy(dev_A, A, size, cudaMemcpyHostToDevice);
    cudaMemcpy(dev_B, B, size, cudaMemcpyHostToDevice);

    // Define block and grid sizes
    dim3 dimBlock(8, 8); // 8×8 threads per block for the 8×8 matrix
    dim3 dimGrid(1, 1);  // Only need one block for 8×8 matrix

    // Timing GPU kernel execution
    cudaEvent_t start, stop;
    cudaEventCreate(&start);
    cudaEventCreate(&stop);
    cudaEventRecord(start);

    // Launch kernel
    matmul<<>>(dev_A, dev_B, dev_C, N);

    // Check for kernel launch errors
    cudaError_t err = cudaGetLastError();
    if (err != cudaSuccess) {
        std::cout << "CUDA kernel launch error: " << cudaGetErrorString(err) << std::endl;
        cudaFree(dev_A);
        cudaFree(dev_B);
        cudaFree(dev_C);
        free(A);
        free(B);
        free(C);
        return -1;
    }

    // Add explicit synchronization
    cudaDeviceSynchronize();

    cudaEventRecord(stop);
    cudaEventSynchronize(stop);

    float milliseconds = 0;
    cudaEventElapsedTime(&milliseconds, start, stop);

    // Copy result back to CPU
    cudaMemcpy(C, dev_C, result_size, cudaMemcpyDeviceToHost);

    // Print the resulting matrix C
    std::cout << "\nMatrix C (Result):" << std::endl;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            std::cout << C[i * N + j] << " ";
        }
        std::cout << std::endl;
    }

    // Print execution time
    std::cout << "\nMatrix multiplication completed in " << milliseconds << " ms\n";

    // Verify CPU calculation for comparison
    std::cout << "\nCPU Verification:" << std::endl;
    for (int i = 0; i < N; i++) {
        for (int j = 0; j < N; j++) {
            long long int value = 0;
            for (int k = 0; k < N; k++) {
                value += (long long int)A[i * N + k] * B[k * N + j];
            }
            std::cout << value << " ";
        }
        std::cout << std::endl;
    }

    // Free memory
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaFree(dev_C);
    free(A);
    free(B);
    free(C);

    return 0;
}
"""

with open('mat_multi.cu', 'w') as f:
    f.write(cuda_code)

     

!nvcc mat_multi.cu -o mat_multi -arch=sm_75 # -arch=sm_75 signifies For CUDA 12.5 with a T4 GPU (compute capability 7.5),
     

!./mat_multi


//OUTPUT     
Matrix A:
1 2 3 4 5 6 7 8 
9 10 11 12 13 14 15 16 
17 18 19 20 21 22 23 24 
25 26 27 28 29 30 31 32 
33 34 35 36 37 38 39 40 
41 42 43 44 45 46 47 48 
49 50 51 52 53 54 55 56 
57 58 59 60 61 62 63 64 

Matrix B:
1 0 0 0 0 0 0 0 
0 1 0 0 0 0 0 0 
0 0 1 0 0 0 0 0 
0 0 0 1 0 0 0 0 
0 0 0 1 1 0 0 0 
0 0 0 0 0 1 0 0 
0 0 0 0 0 0 1 0 
0 0 0 0 0 0 0 1 

Matrix C (Result):
1 2 3 9 5 6 7 8 
9 10 11 25 13 14 15 16 
17 18 19 41 21 22 23 24 
25 26 27 57 29 30 31 32 
33 34 35 73 37 38 39 40 
41 42 43 89 45 46 47 48 
49 50 51 105 53 54 55 56 
57 58 59 121 61 62 63 64 

Matrix multiplication completed in 0.106496 ms

CPU Verification:
1 2 3 9 5 6 7 8 
9 10 11 25 13 14 15 16 
17 18 19 41 21 22 23 24 
25 26 27 57 29 30 31 32 
33 34 35 73 37 38 39 40 
41 42 43 89 45 46 47 48 
49 50 51 105 53 54 55 56 
57 58 59 121 61 62 63 64 


     


     